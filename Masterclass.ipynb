{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "akVPzmUw5MdP"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdwpUFadusExWqzIn1mFg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emiliawisnios/Masterclass_Text_Analysis/blob/main/Masterclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Large Language Models (LLMs) and Natural Language Processing (NLP) Tasks\n",
        "\n",
        "Welcome to this interactive notebook on LLMs and their applications in NLP tasks! This notebook is designed to complement the workshop on \"Introduction to large language models (LLMs) implementations into NLP tasks\" and provide hands-on experience with LLMs.\n",
        "\n",
        "## Objective\n",
        "The primary goal of this notebook is to bridge the gap between theoretical understanding and practical implementation of LLMs in text analysis tasks. By the end of this session, you will have gained practical experience in:\n",
        "\n",
        "1. Understanding the basics of LLMs\n",
        "2. Implementing various prompting techniques\n",
        "3. Applying LLMs to simple NLP tasks\n",
        "4. Evaluating LLM performance on different tasks\n",
        "\n",
        "## Structure\n",
        "This notebook is divided into several sections, each focusing on a different aspect of LLMs and their implementation:\n",
        "\n",
        "- **Environment Setup**: We'll begin by setting up our working environment with the necessary libraries and tools.\n",
        "- **Dataset Preparation**: We'll load and prepare a dataset for our NLP tasks.\n",
        "- **Data Annotation**: You'll get hands-on experience with data annotation, a crucial step in many NLP tasks.\n",
        "- **Model Selection**: We'll explore how to choose and load an appropriate LLM for our tasks.\n",
        "- **Prompting Techniques**: We'll dive into various prompting methods, including:\n",
        "  - Zero-shot prompting\n",
        "  - One-shot prompting\n",
        "  - Few-shot learning\n",
        "  - Chain-of-thought prompting\n",
        "\n",
        "- **Parameter Selection**: We'll experiment with different model parameters and their effects on outputs.\n",
        "- **Reporting and Evaluation**: Finally, we'll look at how to report and evaluate the results of our \"LLM implementations\".\n",
        "\n",
        "By working through this notebook, you'll gain practical insights into how LLMs can be leveraged for various NLP tasks, complementing the theoretical knowledge from the workshop. Let's get started on this exciting journey into the world of Large Language Models!"
      ],
      "metadata": {
        "id": "vsyVxM7-3UFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "akVPzmUw5MdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "kpjGYpTw5LnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NcEKJyQ1clbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation"
      ],
      "metadata": {
        "id": "C8a5RfirFmI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/emiliawisnios/Masterclass_Text_Analysis/refs/heads/main/data/UN_intervention.csv"
      ],
      "metadata": {
        "id": "viwACnvFFpQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/UN_intervention.csv')"
      ],
      "metadata": {
        "id": "ORyHS5z_ciid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "EN8e6esYcrRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ = df.sample(30)"
      ],
      "metadata": {
        "id": "X_-WSVudcwdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data annotation"
      ],
      "metadata": {
        "id": "77q0uUxjdEO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task will be to annotate the sentiment of the speaker to the word *intervention* in the context of the sentence. Run the cell below and write\n",
        "- `NEG` for negative sentinemt to the word *intervention*\n",
        "- `NET` for neutral sentiment to the word *intervention*\n",
        "- `POS` for positive sentiment to the word *intervention*"
      ],
      "metadata": {
        "id": "kWGgpfwLdHfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "xeM_9JWlg0Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = []\n",
        "for index, row in df_.iterrows():\n",
        "  display(HTML(f\"<div>{row['sentence']}</div>\"))\n",
        "  sentiment.append(input())\n",
        "\n",
        "df_['sentiment'] = sentiment"
      ],
      "metadata": {
        "id": "LjmjEVewgC7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_.head()"
      ],
      "metadata": {
        "id": "CnG6M7iPgKL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save and DOWNLOAD the result (in case of session disrutption)**"
      ],
      "metadata": {
        "id": "Kawr0p97-7Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_.to_csv('sentiment_annotated.csv', index=False)"
      ],
      "metadata": {
        "id": "u8gPmnic_EKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Hugging Face (HF) token"
      ],
      "metadata": {
        "id": "Na3V7pTN5zsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the HF token you have to log in to the website [here](https://huggingface.co/). Go to Profile -> Settings -> Access Tokens -> Create new token.\n",
        "\n",
        "Write token name and select option -> *Read access to contents of all public gated repos you can access*. This option should be enough for today's class.\n",
        "\n",
        "Paste the token in the cell below.\n",
        "\n",
        "**REMEMBER NEVER TO SHARE YOUR SECRET TOKENS ANYWHERE, INCLUDING GITHUB.**"
      ],
      "metadata": {
        "id": "DO0s6W2h5360"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HF token\n",
        "HF_TOKEN='' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "oPdKevjq52K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection"
      ],
      "metadata": {
        "id": "y8FpZKwm5ZGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's class we'll be using open-source model. We've chosen the smallest one from the Llama3.2 family due to our constraints in computational resources."
      ],
      "metadata": {
        "id": "0MFTKPd1_K99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by downloading the model:"
      ],
      "metadata": {
        "id": "uZUQ2wai_qfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXd9MME53L96"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    token=HF_TOKEN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run some sample code to check if everyting is working:"
      ],
      "metadata": {
        "id": "FxrcMWWk_pEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ],
      "metadata": {
        "id": "Z775Kst4_pmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!\n",
        "\n",
        "Now let's talk about three basic roles in LLM prompting:\n",
        "- `system`: A system prompt is essentially an instruction or a set of instructions you provide to an LLM before you give it a specific task or question.\n",
        "Think of it as setting the stage for the AI model. You're giving it a role, context, or guidelines to follow.\n",
        "\n",
        "*System prompts are crucial for several reasons:*\n",
        "\n",
        "**Guidance and Context**: They tell the LLM how to approach the task. For example, you might instruct it to be concise, detailed, formal, informal, or adopt a specific persona.\n",
        "\n",
        "**Consistency of Output**: By providing clear instructions, you increase the chances of getting consistently relevant and appropriate responses.\n",
        "\n",
        "**Consistency of Output**: By providing clear instructions, you increase the chances of getting consistently relevant and appropriate responses."
      ],
      "metadata": {
        "id": "QHoqpF7J_6NK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `user`: The user provides the prompt, which can be a question, a command, a statement, or even a piece of code. This input is what triggers the LLM to generate a response. The user's prompts steer the conversation or task. Follow-up questions, clarifications, and new instructions all come from the user, shaping the direction of the interaction."
      ],
      "metadata": {
        "id": "pxMPBDyEDK3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `assistant`: It's the role the LLM takes on during the interaction."
      ],
      "metadata": {
        "id": "p4sO4UKtDurJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example with OpenAI API\n",
        "\n",
        "Another way is to use paid API from some company, for example OpenAI, Anthropic or Google. Below is an example of code doing the same thing as above but with OpenAI API.\n",
        "\n",
        "\n",
        "```\n",
        "import openai\n",
        "\n",
        "# Set up your OpenAI API key\n",
        "openai.api_key = 'your-openai-api-key'\n",
        "\n",
        "# Define messages for the conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "# Call the OpenAI API to generate a response\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",  # Or \"gpt-4\" if you want to use GPT-4\n",
        "    messages=messages,\n",
        "    max_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    frequency_penalty=1.15\n",
        ")\n",
        "\n",
        "# Print the generated response (from the assistant)\n",
        "print(response['choices'][0]['message']['content'])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "CGV715qUCo86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot prompting\n",
        "\n",
        "Now your task will be to find the best possible prompt to assign sentiment to the word `intervention`.\n",
        "\n",
        "You will focus on system prompt for now. Try to state the task clearly. Do not give additional examples - we will try that later."
      ],
      "metadata": {
        "id": "9hHmisODFs9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = 'Your task is to classify the sentiment of the speaker to the word intervention. Response only with NEG (for negative sentiment), NET (for neutral sentiment), POS (for positive sentiment).'\n",
        "prompt2 = ''\n",
        "prompt3 = ''\n",
        "prompt4 = ''\n",
        "prompt5 = ''\n",
        "prompts = [prompt1, prompt2, prompt3, prompt4, prompt5]"
      ],
      "metadata": {
        "id": "ehhL6X1R_5h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_zero_shot = df_.copy()\n",
        "i = 0\n",
        "\n",
        "for prompt in prompts:\n",
        "  i += 1\n",
        "  answers = []\n",
        "  for index, row in df_zero_shot.iterrows():\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": row['sentence']},\n",
        "    ]\n",
        "    outputs = pipe(\n",
        "        messages,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    answers.append(outputs[0][\"generated_text\"][-1]['content'])\n",
        "\n",
        "  df_zero_shot[f'prompt{i}'] = answers\n",
        "  print(f'prompt{i} done')\n",
        "\n",
        "\n",
        "prompt_to_acc = {}\n",
        "for i, prompt in enumerate(prompts):\n",
        "  prompt_to_acc[f'prompt{i+1}'] = (df_zero_shot['sentiment'] == df_zero_shot[f'prompt{i+1}']).mean()\n",
        "\n",
        "for k, v in prompt_to_acc.items():\n",
        "  print(f'{k} - {v}')"
      ],
      "metadata": {
        "id": "nbUhcsB7GhfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now display the dataframe, check the answers for each of the prompt.\n",
        "\n",
        "Where are the mistakes? Was your instruction clear enough? Or did you forget to specify some cases?\n",
        "\n",
        "What do you think could help the model understand the task better?"
      ],
      "metadata": {
        "id": "sZmbeziH_NDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "## YOUR CODE GOES HERE ##\n",
        "#########################"
      ],
      "metadata": {
        "id": "HtrcVO1PGn8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-shot prompting\n",
        "\n",
        "The task is always easier if you have some example - it is also true with LLMs. In one-shot prompting we will simulate one round of the conversation with the model as in the example below:"
      ],
      "metadata": {
        "id": "kqfN-Uk0Hg_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Your task is to return a second letter of a surname given. Return only the letter.\"},\n",
        "    {\"role\": \"user\", \"content\": \"John Smith\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"m\"},\n",
        "    {\"role\": \"user\", \"content\": \"Adam Nowak\"}\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ],
      "metadata": {
        "id": "rvZJBcK8H0oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now let's try something similar with our task. Remember NOT to use examples from the evaluation dataset."
      ],
      "metadata": {
        "id": "l9MmiwH8IPRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ''"
      ],
      "metadata": {
        "id": "qPrriHZgAcgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_one_shot = df_.copy()\n",
        "\n",
        "answers = []\n",
        "for index, row in df_one_shot.iterrows():\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": prompt},\n",
        "      {\"role\": \"user\", \"content\": 'TODO'},\n",
        "      {\"role\": \"assistant\", \"content\": 'TODO'},\n",
        "      {\"role\": \"user\", \"content\": row['sentence']},\n",
        "\n",
        "  ]\n",
        "  outputs = pipe(\n",
        "      messages,\n",
        "      max_new_tokens=256,\n",
        "      temperature=0.01,\n",
        "  )\n",
        "  answers.append(outputs[0][\"generated_text\"][-1]['content'])\n",
        "\n",
        "df_one_shot[f'prompt'] = answers\n",
        "\n",
        "\n",
        "print((df_one_shot['sentiment'] == df_one_shot['prompt']).mean())"
      ],
      "metadata": {
        "id": "XqX4nMWhIesk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again check for the mistakes, where are the problems now?"
      ],
      "metadata": {
        "id": "cuRYNPSTAi3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "## YOUR CODE GOES HERE ##\n",
        "#########################"
      ],
      "metadata": {
        "id": "cBT6WJ2eAnc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot learning\n",
        "\n",
        "Sometimes one example is not enough, especially of the task is complicated. We can simulate more rounds of conversation with the model. Please note, that the better are the most diverse examples, so the model could get the essence of the task. Let's try first with our dummy task."
      ],
      "metadata": {
        "id": "rr0RE2ZmIiHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Your task is to return a second letter of a surname given. Return only the letter.\"},\n",
        "    {\"role\": \"user\", \"content\": \"John Smith\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"m\"},\n",
        "    {\"role\": \"user\", \"content\": \"Adam Nowak\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"o\"},\n",
        "    {\"role\": \"user\", \"content\": \"Maria Krzywa\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"r\"},\n",
        "    {\"role\": \"user\", \"content\": \"Jan Pieprz\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"i\"},\n",
        "    {\"role\": \"user\", \"content\": \"Izabela Lecka\"}\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=1\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ],
      "metadata": {
        "id": "Fs7hVB7AJPCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the model understood the task now, let's see if it manages also with our main task."
      ],
      "metadata": {
        "id": "rSonzLTGKLFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ''"
      ],
      "metadata": {
        "id": "gGyrBiFSKUkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_few_shot = df_.copy()\n",
        "\n",
        "answers = []\n",
        "for index, row in df_few_shot.iterrows():\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": prompt},\n",
        "      {\"role\": \"user\", \"content\": 'TODO'},\n",
        "      {\"role\": \"assistant\", \"content\": 'TODO'},\n",
        "      {\"role\": \"user\", \"content\": 'TODO'},\n",
        "      {\"role\": \"assistant\", \"content\": 'TODO'},\n",
        "      {\"role\": \"user\", \"content\": 'TODO'},\n",
        "      {\"role\": \"assistant\", \"content\": 'TODO'},\n",
        "      {\"role\": \"user\", \"content\": 'TODO'},\n",
        "      {\"role\": \"assistant\", \"content\": 'TODO'},\n",
        "      {\"role\": \"user\", \"content\": row['sentence']},\n",
        "\n",
        "  ]\n",
        "  outputs = pipe(\n",
        "      messages,\n",
        "      max_new_tokens=256,\n",
        "      temperature=0.01,\n",
        "  )\n",
        "  answers.append(outputs[0][\"generated_text\"][-1]['content'])\n",
        "\n",
        "df_few_shot[f'prompt'] = answers\n",
        "\n",
        "\n",
        "print((df_few_shot['sentiment'] == df_few_shot['prompt']).mean())"
      ],
      "metadata": {
        "id": "M1Y3uNrdAt-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What where the mistakes now? Maybe you need more examples for complicated cases?"
      ],
      "metadata": {
        "id": "Llznuq7hBAk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "## YOUR CODE GOES HERE ##\n",
        "#########################"
      ],
      "metadata": {
        "id": "i37VkqSyBHj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain-of-though prompting\n",
        "\n",
        "For more analytical tasks, we can also try a method called chain-of thought. Implementation is easy - we just need to add '*Let's think step by step.*' To our prompt. Be careful though - it is proven that it is most effective with areas like mathematics."
      ],
      "metadata": {
        "id": "PJjxSBg7KjdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Your task is to calculate the sum of two numbers.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is 11 + 7?\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.01\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "1OOvviD6LU2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Your task is to calculate the sum of two numbers.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is 11 + 7? Let's think step by step.\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.01\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "cCnsPB2xLu-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now check on one example if this type of reasoning would help in our case."
      ],
      "metadata": {
        "id": "6f_iQLAYBL7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"TODO\"},\n",
        "    {\"role\": \"user\", \"content\": \"TODO\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.01\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "lIANqOveBTK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter selection"
      ],
      "metadata": {
        "id": "575D266yBhcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pOZnrns-BkFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task will be to ask the model to summarize the text, play with the parameters."
      ],
      "metadata": {
        "id": "V3wbwkCfFgIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"It is with gratitude and pride that I come here today to express the voice of the Italian people, a generous and responsible people, who every day have shown their commitment to saving the lives of hundreds of their brothers and sisters in the heart of the Mediterranean region. This Hall calls for profound thinking rather than slogans. In every part of the world, political life is more and more fixated on the present. It is tied to discussions shaped by 24/7 news stations, the Internet and social media. Let me be clear. I belong to a generation for whom social media represent an extraordinary tool, a horizon of freedom that allows us to change lives and prospects. There is nevertheless the risk of reducing that horizon to a discussion of the next opinion poll or tweet. I think we should reject what has become a dictatorship of the moment, and take the time to pay homage to this Hall for its efforts in engaging in more meaningful reflection. I am thinking of my country, which, on a map, appears to be shaped like a bridge, a bridge connecting North and South, Europe and Africa and East and West, a bridge that spans from the Middle East to the Balkans. Because of its geography, and especially its culture, Italy has always been a kind of extraordinary cultural laboratory affected by influences of every kind. That is the reason that we were the first country in Europe to grasp the momentous dimension of what is happening in the Mediterranean region. From the very beginning — even in this Hall last year — we have said that the refugee question is not a question of numbers (see A/69/PV.9). The problem of migration is not one of organization or statistics. The problem is fear, the fear that runs through our societies and that we must take seriously if we wish to defeat it. In Greek mythology, Phobos was the god of fear, able to paralyse the best armies and cause the most easily fought battles to be lost. That is why the glorious and ancient city of Sparta built a great temple to Phobos and did everything to gain his favour. Europe was born to defeat fear and replace it with the ideal of courage, peace, cooperation and civilization. And for a long time, Europe embodied that ideal. Over the past 70 years, our continent has left behind centuries of war and civil war. Europe had become a true miracle. For those like me, who as a young man witnessed the fall of the Berlin Wall and found in that event a reason to devote my life to public service, to see new walls going up today is intolerable. Europe was reborn to tear down walls, not to build them. That is why Italy is on the front line in rescuing thousands of migrants fleeing from war and despair. For that same reason, I had the privilege of accompanying Secretary-General Ban Ki-moon on one of our ships currently participating in the rescue operations. Addressing migratory flows requires the capacity to respond to that emergency with a global and comprehensive strategy. In that vein, Italy has partnered with African countries through a broad array of initiatives, and in particular with the African Union, a cooperation about which I an the opportunity to speak recently at the third International Conference on Financing for Development, held in Addis Ababa, which produced the Addis Ababa Action Agenda. In the 70 years since the birth of the Charter of the United Nations in San Francisco, the Organization has learned how crucial its role is. It has had the wisdom to recognize its mistakes and the strength to correct them by writing a new chapter that will ensure a better future for all of our children. I think that it will take an effort on the part of everyone, and Italy will not shirk from its responsibilities. That is why we decided to present our candidature to the Security Council for a non-permanent seat for 2017-2018, with the ideal in mind of building the peace 48/51 15-29431\n",
        " 29/09/2015 A/70/PV.16 of tomorrow. We believe that it is the job of each one of us here today to create an alternative to the culture of violence and nihilism exemplified by the recent crises in the Mediterranean, the Middle East, Europe and at the borders of Europe. I am thinking, for example, of the consolidation of the ceasefire in Ukraine. I am thinking of the great joy with which we welcomed the news of the agreement between the United States and Cuba, one of extraordinary historic proportions. I am thinking of the hope that each of us now has as a consequence of the agreement with Iran on that country’s nuclear programme, which begins a new hopeful phase. While we are committed to the implementation of that agreement, we also firmly reiterate the right of the people and the State of Israel to exist. Only through dialogue and negotiation will we be able to find a future for coming generations. Moreover, on the delicate question of Israel and Palestine, there is no alternative to dialogue. It is essential to return to the negotiating table, with the goal of reaching a solution based on two States living side-by-side in peace and security. This open debate of the General Assembly has been characterized by many discussions on Syria. All of us have acknowledged and felt, on a very personal level, the failure that years of inertia has produced. We believe that the only way out of that quagmire is through a political solution that leads to a process of genuine transition. That will work only if we have the courage to stare reality in the face and acknowledge the presence of an enemy of unprecedented danger at our doors, namely, Daesh, the embodiment of extremism and terrorism. Through its Carabinieri corps, which plays an important role in Italy and the world, Italy is proud to lead the coalition for training the Iraqi police force. We know that the work of the security forces is decisive in ensuring daily security, enabling a family to return home without incident and enabling a mother to reassure her children. We will continue working with the global coalition to counter ISIL, in particular the United States and Saudi Arabia, and will maintain our leadership role in the working group to counter financing for Daesh. At the same time, we underscore that Daesh is not limited to the specific region of the Middle East, even if there is an extraordinary mosaic of pluralism and beauty there. Daesh may reaffirm itself with strength in Africa, starting with Libya. From this rostrum, I renew my appeal to all the parties who hope for peace and a unified nation in Libya. We must unite our forces. Our Libyan brothers and sisters must know that they are not alone, that the General Assembly has not forgotten them. Italy is ready to collaborate with a national unity Government and to restore cooperation in key areas so as to give Libya back its future. If the new Libyan Government asks us, Italy is ready to take on the leadership role in a mechanism, authorized by the international community, to assist in the stabilization of the country. There are many reasons for our role in the fight against terrorism. It is a battle for values, a battle for culture. The terrorists want us to die. Failing that, they want us to live under their rules. That is why the battle that we are waging is a battle against darkness and fear, because fear is the playground of terrorism. The first area in which we see that is that of culture. When terrorists attack Palmyra or the Bardo Museum in Tunisia, or a school or a university, from Asia to Africa, they are not attacking the past, they are targeting our future. Italy is the country where the culture of the conservation of cultural assets was born. Proud of our roots and of our Renaissance, we have the highest concentration of UNESCO cultural heritage sites in the world. That is why, together with our partner countries and friends, we aspire to be the guardians of culture throughout the world, carrying out concrete actions, both here in New York and at UNESCO headquarters in Paris, through United4Heritage, the Blue Helmets of culture. On the basis of a model developed in our country, we are proposing the establishment of an international task force, with military and civilian members, for operations to protect and rebuild art historical sites. That is our identity. That task force will be available to UNESCO, and it could be deployed in the framework of United Nations peacekeeping operations. Let us not forget that even Europe runs the risk, in the absence of a major educational project that would show that the evil seed of terrorism is growing on European soil as well. Let us not forget that what has happened in recent months and weeks — from Charlie Hebdo in Paris to what took place in Belgium and in Denmark — involved women and men born in European countries, raised and educated in European countries and yet transformed into terrorists who sought to undermine human rights and the very raison d’être of our continent. I think, therefore, that it is important that we all succeed in this educational challenge together and that our peacekeeping model, 15-29431 49/51\n",
        " A/70/PV.16 29/09/2015 which President Obama noted yesterday and for which we thank him, can serve as an established model that can be in deployed in various countries, such as is happening now in Afghanistan. I wish to recall Italy’s commitment to honouring the women and men who have sacrificed their lives for our collective security, in particular in that country. We are proud of the work of our soldiers and civilians aimed at supporting the Afghan Government on the road to peace and prosperity. The Security Council is at the centre of the challenge. That is not a bureaucratic issue, but rather a political one. The Uniting for Consensus group is ready to continue to work with all members. Human rights, which are today under attack, are for us a reference point at every level. I am thinking about Security Council resolution 1325 (2000), on women and peace and security. I am thinking of resolution 69/186 adopted by the General Assembly last December, with its moratorium on the death penalty, an issue on which we will tirelessly work. I also recall the words that the Holy Father Pope Francis pronounced here (see A/70/PV.3) and at the United States Congress. The resolutions against forced and early marriage (resolution 68/148) and against female genital mutilation (resolution 67/146) are clear signs of the shared commitment of our world community. The deep connection between peace and security and between human rights and development is also the message of the current Universal Exposition in Milan. The slogan of Expo 2015, “Feeding the Planet: Energy for Life”, is a message that brings together many of the aspirations of the General Assembly, in particular that of promoting sustainable agriculture. I wish to make a commitment, especially to the African countries, that we will never stop working in that direction, bolstered by Italian know-how and the desire to work together. Guaranteeing access to food for all, fighting world hunger, changing consumption patterns, ensuring the centrality of women as central stakeholders in agriculture, defending smallholder farmers, as well as easing tensions and conflicts caused by the degradation of arable land and the scarcity of water for irrigation, are not secondary issues. The legacy of Expo Milan is assured by the Charter of Milan and by the commitments of each of us to fight climate change. Italy stands alongside Secretary- General Ban Ki-moon and is mobilizing the necessary resources to ensure that the conferences in Lima and Paris are successful. With the adoption of the 2030 Agenda for Sustainable Development (resolution 70/1), Italy has accepted the challenge of the five Ps — people, prosperity, partnership, planet and peace, which we recognize and which inspire our action for the future. But let me say that Italy intends to contribute with strength, in particular in those battles in which some countries seem to be alone. In the next few weeks in Milan, we will welcome our partners, the small island developing States, which are considered small States but are actually great States for their value, to the events on climate-change adaptation that will take place in mid-October at Expo Milan, and we will bring a large delegation to Venice, where we will show participants, in one of the most beautiful artistic cities in the world, how we are working to combat the risks associated with the presence of high waters and the lack of attention on the part of the international community. In conclusion, as a candidate for a non-permanent seat on the Security Council, Italy wants the values we have discussed to occupy a central place in the Security Council. But I do not want us to think of those values in an abstract way. I do not want us to forget that what brings us here is not a document. It is a face; it is many faces. In Italian schools, our children learn about the strong connection among the ancient civilizations of the Mediterranean, Africa and the Middle East. Today, those children are not just extras in a movie. They are the reason why we are here today. We believe that of all the values we teach our children at school we cannot forget that the first value is life. Faced with the migration crisis, many of us were deeply moved this summer by the photo of a little boy named Aylan. He was a child from Kobani, who, together with his older brother, fell asleep without ever being able to see the future. He was photographed, dead, on the beach at Bodrum. We must not limit our commitment to the emotion of the moment. We must bear that image in mind and commit to doing our best. Many children have died in the heart of the Mediterranean. They died on the ships launched in the direction of Europe by traffickers, the new slave traders of today. However, together with all of those children who are no longer with us, I want to recall the names of children whom no one talks about: Yambambi, 50/51 15-29431\n",
        " 29/09/2015 A/70/PV.16 Salvatore, Idris Ibrahim and Francesca Marina. They are some of the children who were born on the ships of the Italian Marines and Coast Guard, which saved thousands of women, and in some cases enabled them to give birth on those ships. I wish to thank my fellow citizens for the extraordinary work that they have carried out. I want their names to be remembered with the names of those who did not make it. Their heroic actions should serve as an admonishment for all of us. Politics can be restored to dignity when we are aware of the enormity of our challenges. The old Europe, born in the name of courage, does not give in to fear. Italy will proudly do its part.\"\"\""
      ],
      "metadata": {
        "id": "6nJnA6zLCUTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature"
      ],
      "metadata": {
        "id": "i9xOb9CXCMqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"TODO\"},\n",
        "    {\"role\": \"user\", \"content\": text},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=\"TODO\"\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "7SogbUzUCXkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the differences of different temperature values?"
      ],
      "metadata": {
        "id": "ONNuI0C8Fzf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-p"
      ],
      "metadata": {
        "id": "5jI_YEbZCX4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"TODO\"},\n",
        "    {\"role\": \"user\", \"content\": text},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=1,\n",
        "    top_p=\"TODO\"\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "bbhlhLlDCfZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the differences of different temperature values?"
      ],
      "metadata": {
        "id": "CSAhVFe4F8dY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repetition penalty"
      ],
      "metadata": {
        "id": "sj-FUhLnCfkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"TODO\"},\n",
        "    {\"role\": \"user\", \"content\": text},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=1,\n",
        "    repetition_penalty=\"TODO\"\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "IbhqlrIsCirF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the differences for different repetition penalty?"
      ],
      "metadata": {
        "id": "51iahYG_GHVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Max new tokens"
      ],
      "metadata": {
        "id": "V6AVFhDaGODo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"TODO\"},\n",
        "    {\"role\": \"user\", \"content\": text},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=\"TODO\",\n",
        "    temperature=1,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "9OIzTwx3GQ3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the differences for different max new tokens?"
      ],
      "metadata": {
        "id": "Rcu9YficGUMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reporting Your Work with LLMs\n",
        "\n",
        "Effectively communicating your work with Large Language Models (LLMs) is crucial in academic and professional settings. This section will guide you through the key elements to include when reporting on LLM-based projects.\n",
        "\n",
        "## 1. Model Specification\n",
        "\n",
        "Clearly state the LLM you used:\n",
        "- Model name and version (e.g., Meta-Llama/Llama-3.2-1B-Instruct, GPT-3.5, BERT-large)\n",
        "- Source/provider of the model (e.g., Meta, OpenAI, Hugging Face)\n",
        "- Model size (number of parameters)\n",
        "- Any fine-tuning or adaptations made to the base model\n",
        "\n",
        "Example:\n",
        "```\n",
        "We used the Meta-Llama/Llama-3.2-1B-Instruct model. This model has 1 billion parameters and was used without any additional fine-tuning.\n",
        "```\n",
        "\n",
        "## 2. Dataset Description\n",
        "\n",
        "Describe the data used in your project:\n",
        "- Source of the data\n",
        "- Size of the dataset (number of samples, tokens, etc.)\n",
        "- Any preprocessing steps applied\n",
        "- If using a benchmark dataset, cite the relevant paper\n",
        "\n",
        "Example:\n",
        "```\n",
        "We used the UN Speeches dataset, which contains 30 annotated sentences. The data was sampled from the original dataset for this experiment.\n",
        "```\n",
        "\n",
        "## 3. Task Description\n",
        "\n",
        "Clearly define the NLP task you're addressing:\n",
        "- Type of task (e.g., sentiment analysis, text classification, summarization)\n",
        "- Input and output format\n",
        "- Any specific constraints or requirements\n",
        "\n",
        "Example:\n",
        "```\n",
        "Our task was sentiment analysis of sentences containing the word \"intervention\". The input was a sentence, and the output was a sentiment label (NEG, NET, or POS) indicating the speaker's sentiment towards the word \"intervention\" in the context of the sentence.\n",
        "```\n",
        "\n",
        "## 4. Prompting Strategy\n",
        "\n",
        "Detail your prompting approach:\n",
        "- Type of prompting used (zero-shot, one-shot, few-shot, chain-of-thought)\n",
        "- Exact prompt templates used\n",
        "- Any iterations or refinements made to the prompts\n",
        "\n",
        "Example:\n",
        "```\n",
        "We experimented with various prompting strategies:\n",
        "\n",
        "1. Zero-shot: \"Analyze the sentiment towards the word 'intervention' in the given sentence. Respond with NEG for negative, NET for neutral, or POS for positive.\"\n",
        "\n",
        "2. Few-shot: We provided 4 examples of sentences with their correct sentiment labels before asking the model to classify a new sentence.\n",
        "\n",
        "3. Chain-of-thought: We added \"Let's think step by step\" to our prompt to encourage more detailed reasoning.\n",
        "```\n",
        "\n",
        "## 5. Hyperparameters\n",
        "\n",
        "Specify the key hyperparameters used:\n",
        "- Temperature\n",
        "- Top-p (nucleus sampling)\n",
        "- Repetition penalty\n",
        "- Max new tokens\n",
        "- Any other relevant parameters\n",
        "\n",
        "Example:\n",
        "```\n",
        "We experimented with different hyperparameter settings:\n",
        "- Temperature: 0.1, 0.5, 1.0\n",
        "- Top-p: 0.9, 0.95, 1.0\n",
        "- Repetition penalty: 1.0, 1.15, 1.3\n",
        "- Max new tokens: 64, 128, 256\n",
        "```\n",
        "\n",
        "## 6. Evaluation Metrics\n",
        "\n",
        "Describe how you evaluated the model's performance:\n",
        "- Metrics used (e.g., accuracy, F1-score, BLEU)\n",
        "- How ground truth was established\n",
        "- Any statistical tests performed\n",
        "\n",
        "Example:\n",
        "```\n",
        "We evaluated our model using accuracy, comparing the model's predictions to human-annotated labels. We calculated accuracy for each prompting strategy and hyperparameter setting.\n",
        "```\n",
        "\n",
        "## 7. Results and Analysis\n",
        "\n",
        "Present your findings:\n",
        "- Overall performance metrics\n",
        "- Comparison of different approaches\n",
        "- Error analysis and challenging cases\n",
        "- Visualizations of results\n",
        "\n",
        "Example:\n",
        "```\n",
        "The few-shot prompting strategy achieved the highest accuracy of 85%. Zero-shot performance was 70%, while chain-of-thought reasoning improved results slightly to 75%. We observed that the model struggled most with neutral sentiment cases.\n",
        "```\n",
        "\n",
        "## 8. Limitations and Biases\n",
        "\n",
        "Discuss any limitations or potential biases in your approach:\n",
        "- Model limitations\n",
        "- Dataset biases\n",
        "- Potential ethical considerations\n",
        "\n",
        "Example:\n",
        "```\n",
        "Our model showed a slight bias towards positive sentiment. The limited size of our dataset (30 samples) may not be representative of all possible use cases. Further testing on a larger, more diverse dataset is recommended.\n",
        "```\n",
        "\n",
        "## 9. Reproducibility Information\n",
        "\n",
        "Provide details to ensure reproducibility:\n",
        "- Seed values for any random processes\n",
        "- Hardware specifications\n",
        "- Software versions\n",
        "- Any additional libraries or tools used\n",
        "\n",
        "Example:\n",
        "```\n",
        "Experiments were run on a Google Colab notebook with a T4 GPU. We used Python 3.8 and the transformers library version 4.28.1. Random seed was set to 42 for all experiments.\n",
        "```\n",
        "\n",
        "## 10. Conclusion and Future Work\n",
        "\n",
        "Summarize your findings and suggest next steps:\n",
        "- Key takeaways from your experiments\n",
        "- Potential improvements or extensions\n",
        "- Broader implications of your work\n",
        "\n",
        "Example:\n",
        "```\n",
        "Our experiments demonstrate the effectiveness of few-shot prompting for sentiment analysis in intervention-related contexts. Future work could explore larger models, more diverse datasets, and the integration of domain-specific knowledge to improve performance.\n",
        "```\n",
        "\n",
        "By including these elements in your report, you provide a comprehensive and transparent account of your work with LLMs, facilitating reproducibility and advancing the field of NLP research."
      ],
      "metadata": {
        "id": "u7J0jo0rCvGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus - Multiple-Choice Questions (MCQs)"
      ],
      "metadata": {
        "id": "pdNt4rEdCyi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "soJAuuJDD0R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Your task select the correct answer to the question. Return only A or B.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Wakanda? \\n Options: \\n A. Warsaw \\n B. Paris \\n Answer:\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "F8PIGRmiC3Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Your task select the correct answer to the question. Return only A or B.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Wakanda? \\n Options: \\n A. Paris \\n B. Warsaw \\n Answer:\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1]['content'])"
      ],
      "metadata": {
        "id": "y6CnhtMIDOSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the options:\n",
        "1. Generate the answers with temperature 0 and report consistency of the answers, meaning if the answers are the same on the permutations.\n",
        "2. Generate answers multiple times (for example 5 per permutation) and raport the consistency of the average of answers."
      ],
      "metadata": {
        "id": "MihxCxcwEqcJ"
      }
    }
  ]
}